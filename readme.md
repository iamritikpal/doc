# Computer Vision Complete Study Guide (Units 1-3)
## ğŸ“š Comprehensive Notes for Mid-Semester Exam

This guide covers all topics from your syllabus with detailed explanations, visual diagrams, mathematical derivations, and solved examples. Perfect for exam preparation!

---

## ğŸ“– Table of Contents
1. [Unit 1: Image Formation Models & Camera Systems](#unit-1-image-formation-models--camera-systems)
2. [Unit 2: Image Processing & Feature Extraction](#unit-2-image-processing--feature-extraction) 
3. [Unit 3: Motion Estimation & 3D Vision](#unit-3-motion-estimation--3d-vision)
4. [Solved Numerical Examples](#solved-numerical-examples)
5. [Step-by-Step Algorithms](#step-by-step-algorithms)
6. [Quick Reference Cheat Sheet](#quick-reference-cheat-sheet)

---

# Unit 1: Image Formation Models & Camera Systems

## 1.1 Imaging Systems Overview

### Monocular vs Binocular Imaging

```
MONOCULAR SYSTEM          BINOCULAR SYSTEM
     (Single Camera)           (Stereo Cameras)

        Object                    Object
          |                        |
          |                   _____|_____
          v                  /           \
      [Camera]              /             \
          |               [Left]         [Right]
          |                 |             |
      [Image]           [Left Image] [Right Image]
                             |             |
                             \             /
                              \___________/
                                   |
                               [Depth Map]
```

**Key Differences:**
- **Monocular**: Single viewpoint, loses depth information
- **Binocular**: Two viewpoints, enables depth perception through disparity

## 1.2 Camera Projection Models

### Perspective Projection (Pinhole Camera)

The fundamental model for most cameras:

```
3D WORLD COORDINATE SYSTEM â†’ 2D IMAGE PLANE

                    Z (optical axis)
                    â†‘
                    |    
                    |    P(X,Y,Z)
                    |   /
                    |  /
                    | /
    Y â†-------------|----------â†’ 
                    |/           
                    C (camera center)
                    |
                    |
                    |___________â†’ X
                    
                    |
                    | f (focal length)
                    |
                ----+----  â† Image plane
                    |
                   p(x,y) â† projected point
```

**Mathematical Formulation:**

**Basic Perspective Equations:**
```
x = f Ã— (X/Z)
y = f Ã— (Y/Z)
```

**Homogeneous Coordinates (Complete Model):**
```
s Ã— [u]   [fx  s  cx] [r11 r12 r13 tx] [X]
    [v] = [ 0 fy  cy] [r21 r22 r23 ty] [Y]
    [1]   [ 0  0   1] [r31 r32 r33 tz] [Z]
                                        [1]
```

Where:
- **Intrinsic Matrix K**: Camera internal parameters
- **Extrinsic Matrix [R|t]**: Camera pose in world coordinates

### Orthographic Projection

Simplified model for distant objects:

```
PERSPECTIVE vs ORTHOGRAPHIC

Perspective:           Orthographic:
    \  |  /               |  |  |
     \ | /                |  |  |
      \|/                 |  |  |
       â€¢  â† pinhole       |  |  |
       |                  |  |  |
   [Image]            [Image]

Size âˆ 1/Z            Size = constant
```

**Mathematical Model:**
```
x = sâ‚“ Ã— X + cx
y = sáµ§ Ã— Y + cy
```

**When to Use:**
- Telephoto lenses (small field of view)
- Objects very far from camera
- Medical imaging, satellite imagery

### Weak Perspective

Compromise between perspective and orthographic:

```
x = Î± Ã— X + cx
y = Î± Ã— Y + cy

where Î± = f/ZÌ„ (average depth)
```

## 1.3 Camera Intrinsic Parameters

### The Intrinsic Matrix K

```
K = [fx  s  cx]
    [ 0 fy  cy]
    [ 0  0   1]
```

**Parameter Meanings:**

1. **fx, fy**: Focal lengths in pixel units
   - fx = f/px (f in mm, px = pixel size in mm)
   - fy = f/py
   - Usually fx â‰ˆ fy for square pixels

2. **cx, cy**: Principal point (optical center)
   - Ideally at image center: (width/2, height/2)
   - Can be offset due to manufacturing

3. **s**: Skew parameter
   - Usually â‰ˆ 0 for modern cameras
   - Non-zero if pixel grid not rectangular

### Lens Distortion

Real lenses introduce distortion:

```
BARREL DISTORTION        PINCUSHION DISTORTION
    (negative kâ‚)            (positive kâ‚)

     ___                         ___
    (   )                       |   |
   (     )          vs         (     )
    (   )                       |   |
     ___                         ---
```

**Distortion Model:**
```
Radial: Î´r = kâ‚rÂ³ + kâ‚‚râµ + kâ‚ƒrâ·
Tangential: Î´x = 2pâ‚xy + pâ‚‚(rÂ² + 2xÂ²)
           Î´y = pâ‚(rÂ² + 2yÂ²) + 2pâ‚‚xy
```

## 1.4 Camera Extrinsic Parameters

### Rotation and Translation

**Extrinsic parameters define camera pose:**
- **R**: 3Ã—3 rotation matrix (3 DOF)
- **t**: 3Ã—1 translation vector (3 DOF)

```
WORLD TO CAMERA TRANSFORMATION

World Point Pw â†’ Camera Point Pc
Pc = R Ã— Pw + t

Or in homogeneous form:
[Pc] = [R  t] [Pw]
[ 1]   [0  1] [ 1]
```

### Euler Angles (ZYX Convention)

**Sequential Rotations:**
1. Rotate by Î³ around Z-axis
2. Rotate by Î² around Y-axis  
3. Rotate by Î± around X-axis

```
Rx(Î±) = [1    0      0   ]    Ry(Î²) = [cos Î²   0  sin Î²]
        [0  cos Î± -sin Î±]              [  0     1    0  ]
        [0  sin Î±  cos Î±]              [-sin Î²  0  cos Î²]

Rz(Î³) = [cos Î³ -sin Î³  0]
        [sin Î³  cos Î³  0]
        [  0     0     1]

Final: R = Rz(Î³) Ã— Ry(Î²) Ã— Rx(Î±)
```

**Gimbal Lock Warning:** Avoid Î² = Â±90Â° in optimization!

## 1.5 Linear Transformations in 2D

### Transformation Types

```
TRANSFORMATION HIERARCHY

Projective (8 DOF)
    |
Affine (6 DOF)
    |
Similarity (4 DOF)  
    |
Euclidean (3 DOF)
    |
Translation (2 DOF)
```

**1. Translation:**
```
T = [1  0  tx]
    [0  1  ty]
    [0  0   1]
```

**2. Rotation:**
```
R = [cos Î¸  -sin Î¸  0]
    [sin Î¸   cos Î¸  0]
    [ 0       0     1]
```

**3. Scaling:**
```
S = [sx  0   0]
    [ 0  sy  0]
    [ 0   0  1]
```

**4. Shearing:**
```
Hx = [1  shx  0]     Hy = [ 1   0   0]
     [0   1   0]          [shy  1   0]
     [0   0   1]          [ 0   0   1]
```

**5. Affine (combines all above):**
```
A = [a11  a12  tx]
    [a21  a22  ty]
    [ 0    0   1]
```

**6. Projective (Homography):**
```
H = [h11  h12  h13]
    [h21  h22  h23]
    [h31  h32  h33]
```

### Homography Between Planes

**Key Application:** Relating two views of the same plane

```
PLANAR HOMOGRAPHY EXAMPLE

View 1:                View 2:
+-------+              +-------+
|   A   |              |  A'   |
| D   B | ----H----->  |D'   B'|
|   C   |              |  C'   |
+-------+              +-------+

A' = H Ã— A (in homogeneous coordinates)
```

**Mathematical Derivation:**
For a plane Ï€: náµ€X + d = 0, the homography between two camera views is:

```
H = K' Ã— (R - tÃ—náµ€/d) Ã— Kâ»Â¹
```

## 1.6 Radiometry and Light

### Basic Radiometric Quantities

```
LIGHT MEASUREMENT CHAIN

Light Source â†’ Surface â†’ Camera
    |           |         |
Radiant      Reflected   Measured
Intensity    Radiance    Irradiance
```

**Key Quantities:**
1. **Radiance L** [WÂ·srâ»Â¹Â·mâ»Â²]: Power per unit area per solid angle
2. **Irradiance E** [WÂ·mâ»Â²]: Power per unit area
3. **Radiant Intensity I** [WÂ·srâ»Â¹]: Power per solid angle

**Important Property:** Radiance is conserved along rays (no absorption)

### Surface Reflection Models

#### Lambertian (Diffuse) Reflection

Perfect diffuse reflector - appears equally bright from all viewing angles.

```
LAMBERTIAN SURFACE

    Light       Normal
      â†˜         â†‘
       â†˜        |
        â†˜       |
    Î¸    â†˜      |
          â†˜_____|
           Surface

Intensity I = Ï Ã— Iâ‚€ Ã— max(0, cos Î¸)
            = Ï Ã— Iâ‚€ Ã— max(0, nâƒ— Â· lâƒ—)
```

Where:
- Ï: albedo (reflectance coefficient)
- Iâ‚€: incident light intensity
- Î¸: angle between surface normal and light direction

#### Specular Reflection (Phong Model)

Shiny surfaces with mirror-like reflection component.

```
SPECULAR REFLECTION

    Light    Normal    View
      â†˜       â†‘        â†—
       â†˜      |       â†—
        â†˜     |      â†—
         â†˜    |     â†—
          â†˜___|____â†—
           Surface
              â†‘
           Reflection

I = ks Ã— Iâ‚€ Ã— max(0, râƒ— Â· vâƒ—)áµ
```

Where:
- ks: specular coefficient
- râƒ—: perfect reflection direction
- vâƒ—: viewing direction  
- m: shininess exponent

**Combined Model:**
```
I_total = I_ambient + I_diffuse + I_specular
        = kaÃ—Ia + kdÃ—Iâ‚€Ã—(nâƒ—Â·lâƒ—) + ksÃ—Iâ‚€Ã—(râƒ—Â·vâƒ—)áµ
```

## 1.7 Color Spaces and Models

### RGB Color Space

**Device-dependent primary colors:**

```
RGB COLOR CUBE

        White(1,1,1)
           +--------+
          /|       /|
    Cyan +--------+ | Magenta
        /  |      / |
       +--------+  |
    Blue|   +-----|--+ Yellow
        |  /      | /
        | /       |/
        +--------+
      Black     Red
     (0,0,0)
```

**Properties:**
- Additive color model
- Device dependent (different monitors â‰  same color)
- Not perceptually uniform

### HSV Color Space

**More intuitive for humans:**

```
HSV CYLINDER

         White
           â†‘
           |  â† Value (brightness)
           |
     ------+------  â† Saturation (purity)
    /             \
   /               \
  /                 \
 /        Hue        \
/     (color angle)   \
+---------------------+
         Black
```

**Conversion from RGB:**
```
V = max(R, G, B)
S = (V - min(R,G,B)) / V  (if V â‰  0)
H = angle in color wheel based on which component is max
```

### YCbCr Color Space

**Separates luminance from chrominance:**

```
Y  = 0.299Ã—R + 0.587Ã—G + 0.114Ã—B  (luminance)
Cb = 0.564Ã—(B - Y)                 (blue chroma)
Cr = 0.713Ã—(R - Y)                 (red chroma)
```

**Advantages:**
- Human vision more sensitive to Y than Cb,Cr
- Enables chroma subsampling for compression
- Used in JPEG, MPEG

## 1.8 Camera Calibration (Zhang's Method)

### Problem Statement

**Goal:** Estimate intrinsic matrix K and extrinsic parameters R,t from images of a known calibration pattern.

```
CALIBRATION SETUP

Known 3D Pattern    â†’    Camera    â†’    Observed 2D Points
(checkerboard)                          
                    
World Coordinates   â†’   Projection  â†’   Image Coordinates
    (X,Y,Z)                               (u,v)
```

### Zhang's Planar Method

**Advantages:**
- Only requires 2D calibration pattern
- Multiple orientations of same pattern
- Robust and widely used

**Step-by-Step Process:**

#### Step 1: Capture Multiple Images
```
IMAGE 1:        IMAGE 2:        IMAGE 3:
+-----+         +-----+         +-----+
| â•±â•²  |         |â•±   â•²|         |    â•±|
|â•±  â•² |   vs    |    â•²|   vs    |   â•± |
|   â•²â•±|         |     â•²|         |  â•±  |
+-----+         +-----+         +-----+
(frontal)       (tilted)        (rotated)
```

#### Step 2: Detect Corners and Estimate Homographies

For each image i, find homography Hi that maps world plane points to image points:

```
[ui]     [Xi]
[vi] = Hi[Yi]  (in homogeneous coordinates)
[1 ]     [1 ]
```

#### Step 3: Solve for Intrinsic Parameters

**Key Insight:** Each homography provides constraints on K.

If Hi = [h1 h2 h3], then:
```
Hi = Î» Ã— K Ã— [r1 r2 t]
```

**Orthogonality Constraints:**
```
r1áµ€ Ã— r2 = 0     (orthogonal)
||r1|| = ||r2||  (unit vectors)
```

These lead to linear equations in matrix B = Kâ»áµ€Kâ»Â¹:

```
[h1áµ€ B h2] = 0
[(h1-h2)áµ€ B (h1-h2)] = 0
```

#### Step 4: Recover K from B

Once B is found, recover K using Cholesky decomposition:
```
B = Kâ»áµ€Kâ»Â¹
Kâ»áµ€ = chol(B)
K = (Kâ»áµ€)â»áµ€
```

#### Step 5: Compute Extrinsics for Each Image

```
Î» = 1/||Kâ»Â¹h1||
r1 = Î» Ã— Kâ»Â¹h1
r2 = Î» Ã— Kâ»Â¹h2
r3 = r1 Ã— r2
t  = Î» Ã— Kâ»Â¹h3
```

#### Step 6: Refine with Bundle Adjustment

Minimize total reprojection error:
```
min Î£áµ¢â±¼ ||páµ¢â±¼ - Ï€(K, Ráµ¢, táµ¢, Pâ±¼)||Â²
```

---

# Unit 2: Image Processing & Feature Extraction

## 2.1 Digital Image Fundamentals

### Image Representation

```
CONTINUOUS vs DISCRETE

Continuous Image f(x,y)    â†’    Discrete Image f[m,n]
                                
     y â†‘                           n â†‘
       |                            |
       |                            |
    ---+--â†’ x                    ---+--â†’ m
                                    
Infinite resolution              Finite grid (MÃ—N pixels)
```

**Sampling Process:**
```
f[m,n] = f(mÃ—Î”x, nÃ—Î”y)
```

Where Î”x, Î”y are sampling intervals.

### Nyquist Sampling Theorem

**Critical Rule:** Sample rate â‰¥ 2 Ã— highest frequency to avoid aliasing

```
ALIASING EXAMPLE

High-freq signal:  âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿
Undersampled:      Â· Â· Â· Â·
Reconstructed:     âˆ¼   âˆ¼   âˆ¼   (false low frequency!)

Proper sampling:   âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿
Sufficient rate:   Â·Â·Â·Â·Â·
Reconstructed:     âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿  (correct!)
```

## 2.2 Fourier Transform in 2D

### Mathematical Definition

**2D Continuous Fourier Transform:**
```
F(u,v) = âˆ«âˆ« f(x,y) Ã— e^(-j2Ï€(ux+vy)) dx dy
```

**2D Discrete Fourier Transform (DFT):**
```
F[k,l] = (1/MN) Ã— Î£â‚˜â‚Œâ‚€^(M-1) Î£â‚™â‚Œâ‚€^(N-1) f[m,n] Ã— e^(-j2Ï€(km/M + ln/N))
```

### Frequency Domain Interpretation

```
SPATIAL vs FREQUENCY DOMAIN

Spatial Domain:              Frequency Domain:
                            
+-------+                   +-------+
|  âˆ©âˆ©   |                   |   Â·   |  â† DC component
| âˆ©âˆ©âˆ©âˆ©  |  â†â†’ DFT â†â†’        | Â·   Â· |  â† Low frequencies  
|âˆ©âˆ©âˆ©âˆ©âˆ©âˆ© |                   |  Â· Â·  |  â† High frequencies
+-------+                   +-------+
(image)                     (spectrum)
```

**Key Properties:**

1. **DC Component:** F[0,0] = average brightness
2. **Low Frequencies:** Smooth variations, overall shape
3. **High Frequencies:** Edges, noise, fine details

### Important Fourier Properties

#### 1. Convolution Theorem
```
f âŠ— h  â†â†’  F Ã— H
(spatial convolution = frequency multiplication)
```

#### 2. Differentiation Property
```
âˆ‚f/âˆ‚x  â†â†’  j2Ï€u Ã— F(u,v)
âˆ‚f/âˆ‚y  â†â†’  j2Ï€v Ã— F(u,v)
```

#### 3. Scaling Property
```
f(ax, by)  â†â†’  (1/|ab|) Ã— F(u/a, v/b)
```

## 2.3 Linear Filtering

### Convolution Operation

**2D Convolution:**
```
g[m,n] = Î£â‚– Î£â‚— f[k,l] Ã— h[m-k, n-l]
       = f âŠ— h
```

**Visual Interpretation:**
```
CONVOLUTION PROCESS

Original Image    Kernel         Result
+---+---+---+    +---+---+      +---+---+---+
| a | b | c |    |-1 | 0 | 1|   |   |   |   |
+---+---+---+  âŠ— +---+---+   = +---+---+---+
| d | e | f |    |-2 | 0 | 2|   |   | R |   |
+---+---+---+    +---+---+      +---+---+---+
| g | h | i |    |-1 | 0 | 1|   |   |   |   |
+---+---+---+    +---+---+      +---+---+---+

R = aÃ—(-1) + bÃ—0 + cÃ—1 + dÃ—(-2) + eÃ—0 + fÃ—2 + gÃ—(-1) + hÃ—0 + iÃ—1
  = -a + c - 2d + 2f - g + i
```

### Common Linear Filters

#### 1. Gaussian Filter (Smoothing)

**1D Gaussian:**
```
G(x) = (1/âˆš(2Ï€ÏƒÂ²)) Ã— e^(-xÂ²/2ÏƒÂ²)
```

**2D Gaussian (Separable):**
```
G(x,y) = G(x) Ã— G(y)

Discrete approximation (Ïƒ=1):
    1  [ 1  2  1 ]
   --- [ 2  4  2 ]
   16  [ 1  2  1 ]
```

**Properties:**
- Removes noise while preserving edges
- Separable (can apply 1D filters sequentially)
- Scale parameter Ïƒ controls blur amount

#### 2. Box Filter (Simple Averaging)

```
Box Filter (3Ã—3):
    1  [ 1  1  1 ]
   --- [ 1  1  1 ]
    9  [ 1  1  1 ]
```

**Frequency Response:**
```
|H(u,v)| = |sin(3Ï€u)/(3sin(Ï€u))| Ã— |sin(3Ï€v)/(3sin(Ï€v))|
```

#### 3. Derivative Filters

**Sobel X-derivative:**
```
Sx = [-1  0  1]     Sy = [-1 -2 -1]
     [-2  0  2]          [ 0  0  0]
     [-1  0  1]          [ 1  2  1]
```

**Scharr (Better rotational symmetry):**
```
Sx = [-3  0  3]     Sy = [-3 -10 -3]
     [-10 0 10]          [ 0   0  0]
     [-3  0  3]          [ 3  10  3]
```

#### 4. Laplacian (Second Derivative)

```
âˆ‡Â²f = âˆ‚Â²f/âˆ‚xÂ² + âˆ‚Â²f/âˆ‚yÂ²

Discrete Laplacian:
    [ 0 -1  0]      or    [-1 -1 -1]
    [-1  4 -1]            [-1  8 -1]
    [ 0 -1  0]            [-1 -1 -1]
```

**Laplacian of Gaussian (LoG):**
```
LoG = âˆ‡Â²(G_Ïƒ âŠ— f) = (âˆ‡Â²G_Ïƒ) âŠ— f
```

## 2.4 Edge Detection

### What Are Edges?

**Definition:** Significant local changes in image intensity

```
EDGE TYPES

Step Edge:     Ramp Edge:     Ridge Edge:
    |              /              /\
    |             /              /  \
----+         ---/           ---/    \---
    |                              
    |                              

Intensity profiles showing different edge characteristics
```

### Canny Edge Detection Algorithm

**Goals:**
1. **Good Detection:** Find all real edges, minimize false positives
2. **Good Localization:** Edges close to true edge locations  
3. **Single Response:** One response per edge

#### Step 1: Noise Reduction

Apply Gaussian smoothing:
```
I_smooth = G_Ïƒ âŠ— I
```

**Effect of Ïƒ:**
- Small Ïƒ: Preserves details, more noise
- Large Ïƒ: Removes noise, loses fine edges

```
Ïƒ = 0.5:  âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿  (noisy but detailed)
Ïƒ = 1.0:  âˆ¼âˆ¼âˆ¼âˆ¼âˆ¼âˆ¼  (balanced)
Ïƒ = 2.0:  âˆ¼  âˆ¼  âˆ¼  (smooth but blurred)
```

#### Step 2: Gradient Computation

```
Gx = Sx âŠ— I_smooth    (horizontal gradients)
Gy = Sy âŠ— I_smooth    (vertical gradients)

Magnitude: M = âˆš(GxÂ² + GyÂ²)
Direction: Î¸ = arctan2(Gy, Gx)
```

**Gradient Direction Quantization:**
```
      90Â°
       |
       |
135Â° â€”â€”+â€”â€” 45Â°
       |
       |
      0Â°

Quantize Î¸ to: {0Â°, 45Â°, 90Â°, 135Â°}
```

#### Step 3: Non-Maximum Suppression

**Goal:** Thin thick edges to single pixel width

```
NMS PROCESS

Before NMS:        After NMS:
                  
  âˆ¿âˆ¿âˆ¿               |
 âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿       â†’      |
  âˆ¿âˆ¿âˆ¿               |
                  
(thick edge)      (thin edge)
```

**Algorithm:**
For each pixel (i,j):
1. Find gradient direction Î¸[i,j]
2. Check neighbors along Î¸ direction
3. Keep pixel only if M[i,j] â‰¥ both neighbors

#### Step 4: Double Thresholding

**Two thresholds:**
- **T_high:** Strong edge threshold
- **T_low:** Weak edge threshold (typically T_low = 0.4 Ã— T_high)

```
THRESHOLDING CLASSIFICATION

M > T_high:   Strong edge (keep)
T_low < M < T_high: Weak edge (maybe keep)
M < T_low:    Non-edge (discard)
```

#### Step 5: Edge Tracking by Hysteresis

**Goal:** Connect weak edges to strong edges

```
HYSTERESIS LINKING

Strong edges: â–ˆâ–ˆâ–ˆâ–ˆ
Weak edges:   â–‘â–‘â–‘â–‘
Background:   ....

Before:                After:
â–ˆâ–ˆâ–ˆâ–ˆ....â–‘â–‘â–‘â–‘           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
....â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘    â†’      ....â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
....â–‘â–‘â–‘â–‘....           ....â–ˆâ–ˆâ–ˆâ–ˆ....
```

**Algorithm:**
1. Start from strong edge pixels
2. Follow connected weak edge pixels
3. Mark connected weak edges as final edges
4. Discard unconnected weak edges

### Canny Parameter Effects

```
PARAMETER TUNING GUIDE

Ïƒ (Gaussian width):
- Too small: Noisy edges
- Too large: Missing fine edges
- Typical: 0.5-2.0

T_high (Strong threshold):
- Too low: Many false edges
- Too high: Missing real edges
- Typical: 0.1-0.3 Ã— max(gradient)

T_low (Weak threshold):
- Usually: 0.4 Ã— T_high
- Lower ratio: More edge linking
- Higher ratio: Less edge linking
```

## 2.5 Corner Detection

### What Are Corners?

**Definition:** Points where image gradient changes direction rapidly

```
FEATURE CLASSIFICATION

Flat Region:    Edge:         Corner:
   ....         ||||           /|
   ....         ||||          / |
   ....         ||||         /  |

No gradient    1D gradient   2D gradient
```

### Harris Corner Detector

#### Mathematical Foundation

**Auto-correlation Function:**
```
E(u,v) = Î£ w(x,y) Ã— [I(x+u, y+v) - I(x,y)]Â²
```

Where w(x,y) is a window function (usually Gaussian).

**Taylor Expansion:**
```
I(x+u, y+v) â‰ˆ I(x,y) + uÃ—Ix + vÃ—Iy

E(u,v) â‰ˆ [u v] Ã— M Ã— [u]
                      [v]
```

**Structure Matrix M:**
```
M = Î£ w(x,y) Ã— [IxÂ²   IxIy]
                [IxIy  IyÂ² ]
```

#### Eigenvalue Analysis

**Matrix M has eigenvalues Î»â‚, Î»â‚‚:**

```
EIGENVALUE INTERPRETATION

Case 1: Î»â‚ â‰ˆ 0, Î»â‚‚ â‰ˆ 0
   â†’ Flat region (no features)

Case 2: Î»â‚ >> Î»â‚‚ â‰ˆ 0  or  Î»â‚‚ >> Î»â‚ â‰ˆ 0
   â†’ Edge (one dominant direction)

Case 3: Î»â‚ >> 0, Î»â‚‚ >> 0
   â†’ Corner (two strong directions)
```

#### Harris Response Function

**Avoid eigenvalue computation:**
```
R = det(M) - k Ã— trace(M)Â²
  = Î»â‚Î»â‚‚ - k(Î»â‚ + Î»â‚‚)Â²

where k âˆˆ [0.04, 0.06]
```

**Response Interpretation:**
```
R > 0:  Corner
R < 0:  Edge  
R â‰ˆ 0:  Flat region
```

#### Complete Harris Algorithm

```
HARRIS CORNER DETECTION PIPELINE

1. Smooth image with Gaussian (Ïƒ â‰ˆ 1)
2. Compute gradients: Ix, Iy
3. For each pixel:
   - Compute structure matrix M (with Gaussian weighting)
   - Calculate R = det(M) - kÃ—trace(M)Â²
4. Apply threshold: keep pixels where R > threshold
5. Non-maximum suppression
6. Return top N corners
```

### Corner Quality Assessment

**Good Corners Have:**
- High Harris response
- Well-distributed in image
- Stable across scales
- Repeatable across viewpoints

```
CORNER QUALITY RANKING

Excellent: â”¼     Good: â•‹      Poor: â•¬
          â”¼           â•‹            â•¬
```

## 2.6 Texture Analysis

### Gray Level Co-occurrence Matrix (GLCM)

**Concept:** Analyze spatial relationships between pixel intensities

```
GLCM CONSTRUCTION

Image:          Direction (1,0):    GLCM:
1 2 1           Pairs: (1,2), (2,1)    
2 1 2           (2,1), (1,2)        i\j  1  2
1 2 1           (1,2), (2,1)        1   [0  4]
                                    2   [4  0]
```

**Common GLCM Features:**
1. **Contrast:** Î£áµ¢â±¼(i-j)Â² Ã— P(i,j)
2. **Energy:** Î£áµ¢â±¼ P(i,j)Â²
3. **Homogeneity:** Î£áµ¢â±¼ P(i,j)/(1+|i-j|)
4. **Correlation:** Measures linear dependency

### Local Binary Patterns (LBP)

**Rotation-invariant texture descriptor:**

```
LBP COMPUTATION

Neighborhood:    Thresholded:    Binary:    LBP Value:
6  5  2         1  1  0         11000111    199
7  6  1    â†’    1  Â·  0    â†’    
9  8  7         1  1  1         

Center = 6, compare neighbors:
5â‰¥6? Yes(1), 2â‰¥6? No(0), 1â‰¥6? No(0), etc.
```

---

# Unit 3: Motion Estimation & 3D Vision

## 3.1 Optical Flow

### Brightness Constancy Assumption

**Fundamental assumption:** Pixel intensities remain constant as they move

```
OPTICAL FLOW CONCEPT

Frame t:           Frame t+1:         Motion Vectors:
  â—                  â—                   â—â†’
    â—                  â—       =           â—â†’
      â—                  â—                   â—â†’

I(x,y,t) = I(x+u, y+v, t+1)
```

### Optical Flow Constraint Equation (OFCE)

**Derivation using Taylor expansion:**
```
I(x+u, y+v, t+1) â‰ˆ I(x,y,t) + uÃ—âˆ‚I/âˆ‚x + vÃ—âˆ‚I/âˆ‚y + âˆ‚I/âˆ‚t

Setting equal (brightness constancy):
uÃ—Ix + vÃ—Iy + It = 0
```

**This is one equation with two unknowns (u,v)!**

### Lucas-Kanade Method

**Solution:** Assume flow is constant in local neighborhood

```
LUCAS-KANADE WINDOW

For each pixel in window W:
uÃ—Ix + vÃ—Iy + It = 0

Matrix form:
[Ixâ‚  Iyâ‚] [u]   [-Itâ‚]
[Ixâ‚‚  Iyâ‚‚] [v] = [-Itâ‚‚]
[... ...] [.]   [...]
[IxN  IyN]       [-ItN]

A Ã— [u,v]áµ€ = b
```

**Least squares solution:**
```
[u] = (Aáµ€A)â»Â¹ Ã— Aáµ€b
[v]

Where:
Aáµ€A = [Î£IxÂ²   Î£IxIy]
      [Î£IxIy  Î£IyÂ² ]
```

**Window Selection:**
```
Good Window:        Bad Window:
â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘            ........
â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘            ........
â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆ            ........
â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆ            ........
(corners/texture)   (uniform)
```

### Pyramidal Lucas-Kanade

**Problem:** Large motions violate small displacement assumption

**Solution:** Multi-scale approach

```
PYRAMID STRUCTURE

Level 2:  [â–“â–“]     â† Coarse level (large motion)
          [â–“â–“]
          
Level 1:  [â–“â–“â–“â–“]   â† Medium level  
          [â–“â–“â–“â–“]
          [â–“â–“â–“â–“]
          [â–“â–“â–“â–“]
          
Level 0:  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] â† Fine level (refinement)
          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
```

**Algorithm:**
1. Build image pyramids for both frames
2. Start at coarsest level, estimate flow
3. Propagate and refine at finer levels
4. Final result combines all levels

## 3.2 Stereo Vision

### Stereo Geometry

```
STEREO CAMERA SETUP

        PL(XL,YL,ZL)  â† World point
           /    \
          /      \
         /        \
    Left Camera  Right Camera
        |           |
        |           |
    [Left Image] [Right Image]
       pL(uL,vL)   pR(uR,vR)
```

**Key Parameters:**
- **Baseline B:** Distance between camera centers
- **Focal length f:** Same for both cameras (rectified)
- **Disparity d:** Horizontal difference d = uL - uR

### Depth from Disparity

**Similar triangles relationship:**
```
Z/f = B/(uL - uR)

Therefore: Z = fÃ—B/d
```

**Depth Resolution:**
```
dZ/dd = -fÃ—B/dÂ²

Implications:
- Close objects: Small d change â†’ Large Z change
- Far objects: Small d change â†’ Small Z change
```

### Stereo Rectification

**Goal:** Make epipolar lines horizontal

```
BEFORE RECTIFICATION       AFTER RECTIFICATION

Left:    Right:           Left:    Right:
  â•±â•²      â•±â•²                â€”â€”      â€”â€”
 â•±  â•²    â•±  â•²               â€”â€”      â€”â€”  
â•±____â•²  â•±____â•²              â€”â€”      â€”â€”

Epipolar lines at angles   Horizontal epipolar lines
â†’ 2D search problem        â†’ 1D search problem
```

### Correspondence Problem

**Challenge:** Match pixels between left and right images

**Constraints:**
1. **Epipolar:** Corresponding points lie on same horizontal line
2. **Ordering:** Relative order preserved (mostly)
3. **Uniqueness:** One-to-one correspondence
4. **Smoothness:** Nearby pixels have similar disparities

**Matching Costs:**
```
SAD: Î£|IL(x,y) - IR(x-d,y)|
SSD: Î£(IL(x,y) - IR(x-d,y))Â²
NCC: Correlation coefficient
```

## 3.3 Structure from Motion

### Fundamental Matrix

**Relates corresponding points in two views:**
```
pâ‚‚áµ€ Ã— F Ã— pâ‚ = 0
```

Where F encodes epipolar geometry.

### Essential Matrix

**For calibrated cameras:**
```
E = [t]Ã— Ã— R

Where [t]Ã— is skew-symmetric matrix of translation
```

### 8-Point Algorithm

**Estimate F from â‰¥8 point correspondences:**
```
For each correspondence (pâ‚áµ¢, pâ‚‚áµ¢):
[uâ‚uâ‚‚  vâ‚uâ‚‚  uâ‚‚  uâ‚vâ‚‚  vâ‚vâ‚‚  vâ‚‚  uâ‚  vâ‚  1] Ã— f = 0

Stack 8+ equations â†’ solve Af = 0 (SVD)
```

---

# Solved Numerical Examples

## Example 1: Camera Calibration

**Problem:** Calibrate camera using checkerboard with 20mm squares.

**Given Data:**
- Image size: 1280Ã—720 pixels
- Principal point estimate: (640, 360)
- 4 corner correspondences:

```
World Points (mm):    Image Points (pixels):
(0, 0)         â†’      (512, 290)
(60, 0)        â†’      (900, 300)  
(60, 40)       â†’      (880, 520)
(0, 40)        â†’      (510, 500)
```

**Solution Steps:**

### Step 1: Estimate Homography H

Set up DLT equations (8 equations from 4 points):
```
For each correspondence (X,Y) â†’ (u,v):
-X  -Y  -1   0   0   0  uX  uY  u
 0   0   0  -X  -Y  -1  vX  vY  v
```

Solving gives (example result):
```
H = [0.85   0.45   512]
    [0.02   1.10   290]
    [8e-4  9e-4    1 ]
```

### Step 2: Extract Camera Parameters

From H = Î»K[râ‚ râ‚‚ t], with K having zero skew and known principal point:
```
K = [fx   0  640]
    [ 0  fy  360]
    [ 0   0    1]
```

**Orthogonality constraints:**
```
râ‚áµ€râ‚‚ = 0
||râ‚|| = ||râ‚‚|| = 1
```

This gives:
```
Î»Â²(Kâ»Â¹hâ‚)áµ€(Kâ»Â¹hâ‚‚) = 0
Î»Â²||Kâ»Â¹hâ‚||Â² = Î»Â²||Kâ»Â¹hâ‚‚||Â²
```

Solving these equations:
```
fx â‰ˆ 1180 pixels
fy â‰ˆ 1170 pixels
```

**Final Result:**
```
K = [1180   0  640]
    [   0 1170  360]
    [   0   0    1]
```

## Example 2: Harris Corner Detection

**Problem:** Compute Harris response for a 3Ã—3 patch.

**Given gradients:**
```
Ix = [1  2  1]    Iy = [1  0 -1]
     [0  0  0]         [2  0 -2]
     [-1-2 -1]         [1  0 -1]
```

**Solution:**

### Step 1: Compute Structure Matrix Components
```
Î£IxÂ² = 1Â² + 2Â² + 1Â² + 0Â² + 0Â² + 0Â² + (-1)Â² + (-2)Â² + (-1)Â²
     = 1 + 4 + 1 + 0 + 0 + 0 + 1 + 4 + 1 = 12

Î£IyÂ² = 1Â² + 0Â² + (-1)Â² + 2Â² + 0Â² + (-2)Â² + 1Â² + 0Â² + (-1)Â²
     = 1 + 0 + 1 + 4 + 0 + 4 + 1 + 0 + 1 = 12

Î£IxIy = (1Ã—1) + (2Ã—0) + (1Ã—-1) + (0Ã—2) + (0Ã—0) + (0Ã—-2) + (-1Ã—1) + (-2Ã—0) + (-1Ã—-1)
      = 1 + 0 - 1 + 0 + 0 + 0 - 1 + 0 + 1 = 0
```

### Step 2: Form Structure Matrix
```
M = [12   0]
    [ 0  12]
```

### Step 3: Compute Harris Response
```
det(M) = 12 Ã— 12 - 0 Ã— 0 = 144
trace(M) = 12 + 12 = 24

With k = 0.05:
R = det(M) - k Ã— trace(M)Â²
  = 144 - 0.05 Ã— 24Â²
  = 144 - 0.05 Ã— 576
  = 144 - 28.8
  = 115.2
```

**Result:** R = 115.2 > 0 â†’ **Strong Corner**

## Example 3: Optical Flow Computation

**Problem:** Compute optical flow using Lucas-Kanade method.

**Given 3Ã—3 window gradients:**
```
Ix values: [2, 1, 3, 2, 4, 1, 3, 2, 1]
Iy values: [1, 3, 2, 4, 1, 3, 2, 1, 4]  
It values: [-2, -1, -3, -2, -4, -1, -3, -2, -1]
```

**Solution:**

### Step 1: Form Normal Equations
```
Aáµ€A = [Î£IxÂ²   Î£IxIy]    Aáµ€b = [-Î£IxIt]
      [Î£IxIy  Î£IyÂ² ]          [-Î£IyIt]
```

### Step 2: Compute Sums
```
Î£IxÂ² = 4+1+9+4+16+1+9+4+1 = 49
Î£IyÂ² = 1+9+4+16+1+9+4+1+16 = 61  
Î£IxIy = 2+3+6+8+4+3+6+2+4 = 38
Î£IxIt = -4-1-9-4-16-1-9-4-1 = -49
Î£IyIt = 2+3+6+8+4+3+6+2+4 = 38
```

### Step 3: Solve System
```
[49  38] [u]   [49]
[38  61] [v] = [38]

Using Cramer's rule:
det = 49Ã—61 - 38Ã—38 = 2989 - 1444 = 1545

u = (49Ã—61 - 38Ã—38)/1545 = (2989-1444)/1545 = 1545/1545 = 1.0
v = (49Ã—38 - 38Ã—49)/1545 = 0/1545 = 0.0
```

**Result:** Flow vector = (1.0, 0.0) pixels â†’ **Horizontal motion**

---

# Step-by-Step Algorithms

## Algorithm 1: Canny Edge Detection

```python
def canny_edge_detection(image, sigma=1.0, t_high=0.2, t_low=0.1):
    """
    Complete Canny edge detection algorithm
    """
    # Step 1: Gaussian smoothing
    gaussian_kernel = create_gaussian_kernel(sigma)
    smoothed = convolve2d(image, gaussian_kernel)
    
    # Step 2: Gradient computation
    sobel_x = [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]
    sobel_y = [[-1, -2, -1], [0, 0, 0], [1, 2, 1]]
    
    grad_x = convolve2d(smoothed, sobel_x)
    grad_y = convolve2d(smoothed, sobel_y)
    
    magnitude = sqrt(grad_xÂ² + grad_yÂ²)
    direction = arctan2(grad_y, grad_x)
    
    # Step 3: Non-maximum suppression
    suppressed = non_maximum_suppression(magnitude, direction)
    
    # Step 4: Double thresholding  
    strong_edges = suppressed > t_high
    weak_edges = (suppressed >= t_low) & (suppressed <= t_high)
    
    # Step 5: Edge tracking by hysteresis
    final_edges = hysteresis_tracking(strong_edges, weak_edges)
    
    return final_edges

def non_maximum_suppression(magnitude, direction):
    """
    Thin edges to single pixel width
    """
    result = zeros_like(magnitude)
    
    # Quantize directions to 0Â°, 45Â°, 90Â°, 135Â°
    angle = direction * 180 / pi
    angle[angle < 0] += 180
    
    for i in range(1, height-1):
        for j in range(1, width-1):
            # Determine neighbors based on gradient direction
            if (0 <= angle[i,j] < 22.5) or (157.5 <= angle[i,j] <= 180):
                neighbors = [magnitude[i,j-1], magnitude[i,j+1]]  # Horizontal
            elif 22.5 <= angle[i,j] < 67.5:
                neighbors = [magnitude[i-1,j-1], magnitude[i+1,j+1]]  # Diagonal /
            elif 67.5 <= angle[i,j] < 112.5:
                neighbors = [magnitude[i-1,j], magnitude[i+1,j]]  # Vertical
            else:  # 112.5 <= angle[i,j] < 157.5
                neighbors = [magnitude[i-1,j+1], magnitude[i+1,j-1]]  # Diagonal \
            
            # Keep pixel if it's maximum along gradient direction
            if magnitude[i,j] >= max(neighbors):
                result[i,j] = magnitude[i,j]
    
    return result
```

## Algorithm 2: Harris Corner Detection

```python
def harris_corner_detection(image, k=0.05, window_size=5, threshold=0.01):
    """
    Complete Harris corner detection
    """
    # Step 1: Gaussian smoothing
    smoothed = gaussian_blur(image, sigma=1.0)
    
    # Step 2: Compute gradients
    grad_x = convolve2d(smoothed, sobel_x_kernel)
    grad_y = convolve2d(smoothed, sobel_y_kernel)
    
    # Step 3: Compute structure matrix components
    Ixx = grad_x * grad_x
    Iyy = grad_y * grad_y  
    Ixy = grad_x * grad_y
    
    # Step 4: Apply Gaussian weighting
    gaussian_window = create_gaussian_kernel(window_size, sigma=1.5)
    
    Sxx = convolve2d(Ixx, gaussian_window)
    Syy = convolve2d(Iyy, gaussian_window)
    Sxy = convolve2d(Ixy, gaussian_window)
    
    # Step 5: Compute Harris response
    det_M = Sxx * Syy - Sxy * Sxy
    trace_M = Sxx + Syy
    harris_response = det_M - k * trace_M * trace_M
    
    # Step 6: Threshold and non-maximum suppression
    corners = harris_response > threshold
    corners = non_maximum_suppression_2d(harris_response, corners)
    
    return corners, harris_response
```

## Algorithm 3: Zhang's Camera Calibration

```python
def zhang_calibration(world_points, image_points_list):
    """
    Zhang's planar calibration method
    """
    homographies = []
    
    # Step 1: Estimate homographies for each view
    for image_points in image_points_list:
        H = estimate_homography_dlt(world_points, image_points)
        homographies.append(H)
    
    # Step 2: Solve for intrinsic parameters
    V = []
    for H in homographies:
        h1, h2, h3 = H[:, 0], H[:, 1], H[:, 2]
        
        # Orthogonality constraints
        v12 = create_v_vector(h1, h2)
        v11_minus_v22 = create_v_vector(h1, h1) - create_v_vector(h2, h2)
        
        V.append(v12)
        V.append(v11_minus_v22)
    
    # Solve Vb = 0 for b (parameters of B = K^-T K^-1)
    V = np.array(V)
    _, _, Vt = svd(V)
    b = Vt[-1, :]
    
    # Recover intrinsic matrix K from b
    K = recover_intrinsics_from_b(b)
    
    # Step 3: Compute extrinsics for each view
    extrinsics = []
    for H in homographies:
        R, t = compute_extrinsics(K, H)
        extrinsics.append((R, t))
    
    # Step 4: Refine all parameters (optional bundle adjustment)
    K_refined, extrinsics_refined = bundle_adjustment(
        K, extrinsics, world_points, image_points_list)
    
    return K_refined, extrinsics_refined

def create_v_vector(h1, h2):
    """Create constraint vector for homography columns h1, h2"""
    return np.array([
        h1[0]*h2[0],
        h1[0]*h2[1] + h1[1]*h2[0],
        h1[1]*h2[1],
        h1[2]*h2[0] + h1[0]*h2[2],
        h1[2]*h2[1] + h1[1]*h2[2],
        h1[2]*h2[2]
    ])
```

---

# Quick Reference Cheat Sheet

## ğŸ“ Essential Formulas

### Camera Projection
```
Perspective:     sÂ·p = K[R|t]P
Orthographic:    p = SÂ·P + c  (S=scaling, c=center)
Homography:      p' = HÂ·p     (planar surfaces)
```

### Intrinsic Matrix
```
K = [fx  s  cx]    fx,fy: focal lengths (pixels)
    [ 0 fy  cy]    cx,cy: principal point  
    [ 0  0   1]    s: skew (usually â‰ˆ 0)
```

### Transformations
```
Rotation:    R = [cos Î¸  -sin Î¸]
                 [sin Î¸   cos Î¸]

Translation: T = [1  0  tx]
                 [0  1  ty] 
                 [0  0   1]

Scaling:     S = [sx  0   0]
                 [ 0 sy   0]
                 [ 0  0   1]
```

### Fourier Transform
```
2D DFT: F[k,l] = Î£â‚˜â‚™ f[m,n]Â·e^(-j2Ï€(km/M + ln/N))
Convolution â†” Multiplication: fâŠ—h â†” FÂ·H
```

### Edge Detection
```
Gradient magnitude: |âˆ‡f| = âˆš(fxÂ² + fyÂ²)
Gradient direction: Î¸ = arctan2(fy, fx)
Laplacian: âˆ‡Â²f = fxx + fyy
```

### Harris Corners
```
Structure matrix: M = [Î£ IxÂ²   Î£ IxIy]
                      [Î£ IxIy  Î£ IyÂ² ]

Harris response: R = det(M) - kÂ·trace(M)Â²
k âˆˆ [0.04, 0.06]
```

### Optical Flow
```
Constraint equation: IxÂ·u + IyÂ·v + It = 0
Lucas-Kanade: [Î£ IxÂ²   Î£ IxIy] [u] = [-Î£ IxIt]
              [Î£ IxIy  Î£ IyÂ² ] [v]   [-Î£ IyIt]
```

### Stereo Vision
```
Depth from disparity: Z = fÂ·B/d
Disparity: d = uL - uR
Baseline: B (distance between cameras)
```

## ğŸ¯ Key Parameter Guidelines

### Canny Edge Detection
```
Ïƒ (Gaussian):     0.5-2.0 pixels
T_high:          0.1-0.3 Ã— max(gradient)
T_low:           0.4 Ã— T_high
```

### Harris Corners
```
k parameter:     0.04-0.06
Window size:     5Ã—5 to 7Ã—7 pixels
Gaussian Ïƒ:      1.0-1.5 pixels
```

### Camera Calibration
```
Minimum images:  3 (practical: 10-20)
Pattern angles:  Vary by 30Â°+ between views
Coverage:        Fill entire image area
```

## ğŸ” Algorithm Checklist

### Canny Steps
1. âœ“ Gaussian blur (noise reduction)
2. âœ“ Sobel gradients (magnitude + direction)  
3. âœ“ Non-maximum suppression (thin edges)
4. âœ“ Double threshold (strong/weak classification)
5. âœ“ Hysteresis tracking (connect weak to strong)

### Harris Steps  
1. âœ“ Gaussian smoothing
2. âœ“ Compute Ix, Iy gradients
3. âœ“ Form structure matrix M (Gaussian weighted)
4. âœ“ Calculate R = det(M) - kÂ·trace(M)Â²
5. âœ“ Threshold and non-maximum suppression

### Zhang Calibration Steps
1. âœ“ Capture multiple checkerboard views
2. âœ“ Detect corners with subpixel accuracy
3. âœ“ Estimate homography for each view (DLT)
4. âœ“ Solve linear system for intrinsics
5. âœ“ Compute extrinsics for each view
6. âœ“ Bundle adjustment refinement

## ğŸ“Š Important Comparisons

| Aspect | Perspective | Orthographic |
|--------|-------------|--------------|
| Rays | Converge at center | Parallel |
| Depth effect | Size âˆ 1/Z | Size constant |
| FOV | Wide angles | Narrow/telephoto |
| Distortion | Foreshortening | None |

| Reflection | Lambertian | Specular |
|------------|------------|----------|
| Dependence | Surface normal | View direction |
| Formula | I âˆ nÂ·l | I âˆ (rÂ·v)áµ |
| Appearance | Matte/diffuse | Shiny/glossy |
| Viewing | Same from all angles | Changes with viewpoint |

| Filter | Sobel | Scharr | Prewitt |
|--------|-------|--------|---------|
| Smoothing | Moderate | Better | Less |
| Isotropy | Good | Excellent | Poor |
| Noise | Moderate | Low | Higher |

---

## ğŸ’¡ Exam Tips

### Formula Memorization
- **Always write homogeneous coordinates** for transformations
- **Remember the k parameter range** for Harris (0.04-0.06)  
- **Canny threshold ratio**: T_low = 0.4 Ã— T_high
- **RGB to grayscale**: 0.299R + 0.587G + 0.114B

### Problem-Solving Strategy
1. **Identify the problem type** (calibration, feature detection, etc.)
2. **List known parameters** and what needs to be found
3. **Choose appropriate method** (Zhang for calibration, Lucas-Kanade for flow)
4. **Show mathematical steps** clearly
5. **Check units and reasonableness** of final answer

### Common Pitfalls
- Forgetting to normalize homogeneous coordinates
- Using wrong coordinate system (image vs. camera vs. world)
- Incorrect matrix dimensions in multiplications
- Not applying non-maximum suppression after thresholding

---

**Good luck with your exam! ğŸ“**

*This study guide covers all major topics from Units 1-3. Practice with the numerical examples and make sure you understand the underlying concepts, not just memorize formulas.*
